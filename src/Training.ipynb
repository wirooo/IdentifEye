{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informal-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Resize, Normalize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import utils\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "configured-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(kind=\"train\"):\n",
    "    if kind==\"train\":\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "driving-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train/valid folders\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import random\n",
    "\n",
    "for pth in [\"../photos/train\", \"../photos/val\"]:\n",
    "    if not os.path.exists(pth):\n",
    "        os.mkdir(pth)\n",
    "\n",
    "img_src = \"../photos/labelled\"\n",
    "\n",
    "d = os.listdir(img_src)\n",
    "img_dir = []\n",
    "for item in d:\n",
    "    if item != \".gitignore\":\n",
    "        img_dir.append(item)\n",
    "train_val_split = 0.85\n",
    "for name in img_dir:\n",
    "    img_path = os.path.join(img_src, name)\n",
    "    imgs = os.listdir(img_path)\n",
    "    for pth in [os.path.join(\"../photos/train\", name), os.path.join(\"../photos/val\", name)]:\n",
    "        if not os.path.exists(pth):\n",
    "            os.mkdir(pth)\n",
    "    for i in range(len(imgs)):\n",
    "#         if i <= len(imgs) * train_val_split:\n",
    "        if random.random() < train_val_split:\n",
    "            # place in test\n",
    "            copyfile(os.path.join(img_path, imgs[i]), os.path.join(\"../photos/train\", name, imgs[i]))\n",
    "        else:\n",
    "            # place in valid\n",
    "            copyfile(os.path.join(img_path, imgs[i]), os.path.join(\"../photos/val\", name, imgs[i]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "muslim-foundation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_src = \"../photos/train\"\n",
    "val_src = \"../photos/val\"\n",
    "\n",
    "train_ds = datasets.ImageFolder(train_src, get_transforms(\"train\"))\n",
    "val_ds = datasets.ImageFolder(val_src, get_transforms(\"val\"))\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=4, shuffle=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pending-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "model_conv = resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "helpful-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Sequential(nn.Linear(in_ftrs, len(os.listdir(img_src)) - 1), nn.Sigmoid())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "opt = optim.Adam(model_conv.fc.parameters(), lr=1e-3)\n",
    "\n",
    "lr_scheduler = lr_scheduler.StepLR(opt, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cooperative-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_fn(model, dataloaders, optimizer, criterion, scheduler, device, num_epochs):\n",
    "    best_acc = -1\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}\", \"-\"*10)\n",
    "        \n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = []\n",
    "            running_acc = []\n",
    "            \n",
    "            for step, (imgs, lbls) in enumerate(dataloaders[phase]):\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(imgs)\n",
    "                    value, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, lbls)\n",
    "                    \n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss.append(loss.item() * imgs.size(0))\n",
    "                running_acc.append((torch.sum(preds == lbls)/ imgs.size(0)).cpu())\n",
    "                \n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = np.mean(running_loss)\n",
    "            epoch_acc = np.mean(running_acc)\n",
    "            print(f\"{phase}, Loss: {epoch_loss}, Acc: {epoch_acc}\")\n",
    "            \n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    return model           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "muslim-election",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ----------\n",
      "train, Loss: 5.328798832992713, Acc: 0.3038194477558136\n",
      "val, Loss: 5.286514317547834, Acc: 0.37037035822868347\n",
      "Epoch 1 ----------\n",
      "train, Loss: 4.9891181116302805, Acc: 0.4861111342906952\n",
      "val, Loss: 4.912916602911772, Acc: 0.5462962985038757\n",
      "Epoch 2 ----------\n",
      "train, Loss: 4.868709382911523, Acc: 0.53125\n",
      "val, Loss: 4.456886730812214, Acc: 0.7314814925193787\n",
      "Epoch 3 ----------\n",
      "train, Loss: 4.605566260715325, Acc: 0.6267361044883728\n",
      "val, Loss: 4.380653908959141, Acc: 0.6388888955116272\n",
      "Epoch 4 ----------\n",
      "train, Loss: 4.475351250420014, Acc: 0.671875\n",
      "val, Loss: 4.188998266502663, Acc: 0.7222222089767456\n",
      "Epoch 5 ----------\n",
      "train, Loss: 4.4128365976115065, Acc: 0.6875\n",
      "val, Loss: 4.129405396955985, Acc: 0.7685185074806213\n",
      "Epoch 6 ----------\n",
      "train, Loss: 4.3265174354116125, Acc: 0.6961805820465088\n",
      "val, Loss: 4.115567840911724, Acc: 0.7037037014961243\n",
      "Epoch 7 ----------\n",
      "train, Loss: 4.327631662289302, Acc: 0.7013888359069824\n",
      "val, Loss: 4.087964314001578, Acc: 0.6481481194496155\n",
      "Epoch 8 ----------\n",
      "train, Loss: 4.302768234163523, Acc: 0.7083333134651184\n",
      "val, Loss: 4.053694883982341, Acc: 0.7129629850387573\n",
      "Epoch 9 ----------\n",
      "train, Loss: 4.350848950445652, Acc: 0.6753472685813904\n",
      "val, Loss: 4.0426091308946965, Acc: 0.7037037014961243\n",
      "Epoch 10 ----------\n",
      "train, Loss: 4.317987175037463, Acc: 0.7083333134651184\n",
      "val, Loss: 4.0085641763828415, Acc: 0.7685185074806213\n",
      "Epoch 11 ----------\n",
      "train, Loss: 4.301591612398624, Acc: 0.6979166865348816\n",
      "val, Loss: 4.029232683005156, Acc: 0.7222222089767456\n",
      "Epoch 12 ----------\n",
      "train, Loss: 4.331333167850971, Acc: 0.711805522441864\n",
      "val, Loss: 4.008553293016222, Acc: 0.75\n",
      "Epoch 13 ----------\n",
      "train, Loss: 4.259923813243707, Acc: 0.7222221493721008\n",
      "val, Loss: 4.027095558466734, Acc: 0.75\n",
      "Epoch 14 ----------\n",
      "train, Loss: 4.149314279357593, Acc: 0.7465277314186096\n",
      "val, Loss: 4.026808537818767, Acc: 0.7222222089767456\n"
     ]
    }
   ],
   "source": [
    "dls = {\"train\": train_dl, \"val\": val_dl}\n",
    "model_conv = train_fn(model_conv.to(device), dls, opt, criterion, lr_scheduler, device, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ranging-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_conv.state_dict(), \"../model_save/model.pth\")\n",
    "np.save(\"../model_save/class_names\", train_ds.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-shoulder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "identifeye_venv37",
   "language": "python",
   "name": "identifeye_venv37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
