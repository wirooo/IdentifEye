{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informal-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Resize, Normalize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import utils\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "configured-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(kind=\"train\"):\n",
    "    if kind==\"train\":\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "driving-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train/valid folders\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import random\n",
    "\n",
    "for pth in [\"../photos/train\", \"../photos/val\"]:\n",
    "    if not os.path.exists(pth):\n",
    "        os.mkdir(pth)\n",
    "\n",
    "img_src = \"../photos/labelled\"\n",
    "\n",
    "train_val_split = 0.85\n",
    "for name in os.listdir(img_src):\n",
    "    img_path = os.path.join(img_src, name)\n",
    "    imgs = os.listdir(img_path)\n",
    "    for pth in [os.path.join(\"../photos/train\", name), os.path.join(\"../photos/val\", name)]:\n",
    "        if not os.path.exists(pth):\n",
    "            os.mkdir(pth)\n",
    "    for i in range(len(imgs)):\n",
    "#         if i <= len(imgs) * train_val_split:\n",
    "        if random.random() < train_val_split:\n",
    "            # place in test\n",
    "            copyfile(os.path.join(img_path, imgs[i]), os.path.join(\"../photos/train\", name, imgs[i]))\n",
    "        else:\n",
    "            # place in valid\n",
    "            copyfile(os.path.join(img_path, imgs[i]), os.path.join(\"../photos/val\", name, imgs[i]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "muslim-foundation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_src = \"../photos/train\"\n",
    "val_src = \"../photos/val\"\n",
    "\n",
    "train_ds = datasets.ImageFolder(train_src, get_transforms(\"train\"))\n",
    "val_ds = datasets.ImageFolder(val_src, get_transforms(\"val\"))\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=4, shuffle=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pending-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "model_conv = resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "helpful-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Sequential(nn.Linear(in_ftrs, len(os.listdir(img_src))), nn.Sigmoid())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "opt = optim.Adam(model_conv.fc.parameters(), lr=1e-3)\n",
    "\n",
    "lr_scheduler = lr_scheduler.StepLR(opt, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cooperative-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_fn(model, dataloaders, optimizer, criterion, scheduler, device, num_epochs):\n",
    "    best_acc = -1\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}\", \"-\"*10)\n",
    "        \n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = []\n",
    "            running_acc = []\n",
    "            \n",
    "            for step, (imgs, lbls) in enumerate(dataloaders[phase]):\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(imgs)\n",
    "                    value, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, lbls)\n",
    "                    \n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss.append(loss.item() * imgs.size(0))\n",
    "                running_acc.append((torch.sum(preds == lbls)/ imgs.size(0)).cpu())\n",
    "                \n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = np.mean(running_loss)\n",
    "            epoch_acc = np.mean(running_acc)\n",
    "            print(f\"{phase}, Loss: {epoch_loss}, Acc: {epoch_acc}\")\n",
    "            \n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    return model           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "muslim-election",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ----------\n",
      "train, Loss: 4.154826879501343, Acc: 0.4047619104385376\n",
      "val, Loss: 4.383073925971985, Acc: 0.5\n",
      "Epoch 1 ----------\n",
      "train, Loss: 4.002050027960823, Acc: 0.5357142686843872\n",
      "val, Loss: 4.285159349441528, Acc: 0.5\n",
      "Epoch 2 ----------\n",
      "train, Loss: 3.8703006903330484, Acc: 0.4523809552192688\n",
      "val, Loss: 3.924210548400879, Acc: 0.5\n",
      "Epoch 3 ----------\n",
      "train, Loss: 3.9341049307868596, Acc: 0.5357142686843872\n",
      "val, Loss: 3.80667781829834, Acc: 0.5\n",
      "Epoch 4 ----------\n",
      "train, Loss: 3.5727114450363886, Acc: 0.6428571343421936\n",
      "val, Loss: 3.722055673599243, Acc: 0.625\n",
      "Epoch 5 ----------\n",
      "train, Loss: 3.506202921980903, Acc: 0.738095223903656\n",
      "val, Loss: 3.4492164850234985, Acc: 0.875\n",
      "Epoch 6 ----------\n",
      "train, Loss: 3.4525029999869212, Acc: 0.6428571343421936\n",
      "val, Loss: 3.2111575603485107, Acc: 1.0\n",
      "Epoch 7 ----------\n",
      "train, Loss: 3.308287751106989, Acc: 0.8095238208770752\n",
      "val, Loss: 3.179069757461548, Acc: 1.0\n",
      "Epoch 8 ----------\n",
      "train, Loss: 3.215711709998903, Acc: 0.8333333134651184\n",
      "val, Loss: 3.2056093215942383, Acc: 1.0\n",
      "Epoch 9 ----------\n",
      "train, Loss: 3.2917400768824985, Acc: 0.761904776096344\n",
      "val, Loss: 3.2286376953125, Acc: 0.875\n",
      "Epoch 10 ----------\n",
      "train, Loss: 3.2725963592529297, Acc: 0.7976190447807312\n",
      "val, Loss: 3.1047433614730835, Acc: 1.0\n",
      "Epoch 11 ----------\n",
      "train, Loss: 3.2526523783093406, Acc: 0.761904776096344\n",
      "val, Loss: 3.148907423019409, Acc: 1.0\n",
      "Epoch 12 ----------\n",
      "train, Loss: 3.25955259232294, Acc: 0.8452380895614624\n",
      "val, Loss: 3.2185187339782715, Acc: 1.0\n",
      "Epoch 13 ----------\n",
      "train, Loss: 3.3490520942778814, Acc: 0.761904776096344\n",
      "val, Loss: 3.2332793474197388, Acc: 1.0\n",
      "Epoch 14 ----------\n",
      "train, Loss: 3.2289419372876487, Acc: 0.8333333134651184\n",
      "val, Loss: 3.1995091438293457, Acc: 1.0\n",
      "Epoch 15 ----------\n",
      "train, Loss: 3.199531600588844, Acc: 0.8333333134651184\n",
      "val, Loss: 3.1467015743255615, Acc: 1.0\n",
      "Epoch 16 ----------\n",
      "train, Loss: 3.274999346051897, Acc: 0.8095238208770752\n",
      "val, Loss: 3.136990189552307, Acc: 1.0\n",
      "Epoch 17 ----------\n",
      "train, Loss: 3.2990886710938954, Acc: 0.7976190447807312\n",
      "val, Loss: 3.181206703186035, Acc: 1.0\n",
      "Epoch 18 ----------\n",
      "train, Loss: 3.1972684179033553, Acc: 0.8571428656578064\n",
      "val, Loss: 3.143962860107422, Acc: 1.0\n",
      "Epoch 19 ----------\n",
      "train, Loss: 3.2523009209405807, Acc: 0.761904776096344\n",
      "val, Loss: 3.1116676330566406, Acc: 1.0\n",
      "Epoch 20 ----------\n",
      "train, Loss: 3.242099656945183, Acc: 0.8214285969734192\n",
      "val, Loss: 3.135457158088684, Acc: 1.0\n",
      "Epoch 21 ----------\n",
      "train, Loss: 3.324964932032994, Acc: 0.738095223903656\n",
      "val, Loss: 3.138274312019348, Acc: 1.0\n",
      "Epoch 22 ----------\n",
      "train, Loss: 3.330784749417078, Acc: 0.75\n",
      "val, Loss: 3.1194950342178345, Acc: 1.0\n",
      "Epoch 23 ----------\n",
      "train, Loss: 3.2960600342069353, Acc: 0.773809552192688\n",
      "val, Loss: 3.133305311203003, Acc: 1.0\n",
      "Epoch 24 ----------\n",
      "train, Loss: 3.2492476190839494, Acc: 0.7857142686843872\n",
      "val, Loss: 3.1544344425201416, Acc: 1.0\n",
      "Epoch 25 ----------\n",
      "train, Loss: 3.346552644457136, Acc: 0.8095238208770752\n",
      "val, Loss: 3.1534301042556763, Acc: 1.0\n",
      "Epoch 26 ----------\n",
      "train, Loss: 3.3047094231560115, Acc: 0.761904776096344\n",
      "val, Loss: 3.1273269653320312, Acc: 1.0\n",
      "Epoch 27 ----------\n",
      "train, Loss: 3.2384806530816213, Acc: 0.8571428656578064\n",
      "val, Loss: 3.195085048675537, Acc: 1.0\n",
      "Epoch 28 ----------\n",
      "train, Loss: 3.102232609476362, Acc: 0.8928571343421936\n",
      "val, Loss: 3.1792516708374023, Acc: 1.0\n",
      "Epoch 29 ----------\n",
      "train, Loss: 3.3374810162044706, Acc: 0.738095223903656\n",
      "val, Loss: 3.150721311569214, Acc: 1.0\n",
      "Epoch 30 ----------\n",
      "train, Loss: 3.227161486943563, Acc: 0.8095238208770752\n",
      "val, Loss: 3.115200161933899, Acc: 1.0\n",
      "Epoch 31 ----------\n",
      "train, Loss: 3.3265518801552907, Acc: 0.761904776096344\n",
      "val, Loss: 3.1105825901031494, Acc: 1.0\n",
      "Epoch 32 ----------\n",
      "train, Loss: 3.2440867878141857, Acc: 0.8333333134651184\n",
      "val, Loss: 3.2046743631362915, Acc: 1.0\n",
      "Epoch 33 ----------\n",
      "train, Loss: 3.2538676886331466, Acc: 0.773809552192688\n",
      "val, Loss: 3.167167901992798, Acc: 1.0\n",
      "Epoch 34 ----------\n",
      "train, Loss: 3.2672371239889237, Acc: 0.75\n",
      "val, Loss: 3.153094172477722, Acc: 0.875\n"
     ]
    }
   ],
   "source": [
    "dls = {\"train\": train_dl, \"val\": val_dl}\n",
    "model_conv = train_fn(model_conv.to(device), dls, opt, criterion, lr_scheduler, device, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ranging-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_conv.state_dict(), \"../model_save/model.pth\")\n",
    "np.save(\"../model_save/class_names\", train_ds.classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "identifeye_venv37",
   "language": "python",
   "name": "identifeye_venv37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
